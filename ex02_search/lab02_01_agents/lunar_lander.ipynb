{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3545c3c6",
   "metadata": {},
   "source": [
    "# Create a Simple Reflex-Based Lunar Lander Agent\n",
    "\n",
    "In this example, we will use Gymnasium, an environment to train agents via reinforcement learning (RL). We will not use RL here but just use the environment with a custom simple reflex-based agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975e747",
   "metadata": {},
   "source": [
    "## Install Gymnasium\n",
    "\n",
    "The documentation for Gymnasium is available at https://gymnasium.farama.org/ \n",
    "\n",
    "Steps:\n",
    "1. Create a new folder and open it with VS Code and install all needed Python Extensions in VS Code.\n",
    "2. Create a new virtual environment (CTRL-Shift P Python Create Environment...)\n",
    "3. I needed to install swig and the Python C++ headers on WSL2 via the terminal\n",
    "    * `sudo apt install swig`\n",
    "    * `sudo apt-get install python3-dev` \n",
    "4. Install gymnasium with the needed extras"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d0bf8a9",
   "metadata": {},
   "source": [
    "%pip install -q swig\n",
    "%pip install -q gymnasium[box2d,classic_control]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a6c9dae",
   "metadata": {},
   "source": [
    "## The Lunar Lander Environment \n",
    "\n",
    "The documentation of the environment is available at: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "* Performance Measure: A reward of -100 or +100 points for crashing or landing safely respectively. We do not use \n",
    "  intermediate rewards here.\n",
    "\n",
    "* Environment: This environment is a classic rocket trajectory optimization problem. A ship needs to land safely. The space is **continuous** with\n",
    "  x and y coordinates in the range [-2.5, 2.5]. The landing pad is at coordinate (0,0).\n",
    "\n",
    "* Actuators:  According to Pontryagin’s\n",
    "  maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off. There are four discrete actions available:\n",
    "\n",
    "    - 0: do nothing\n",
    "    - 1: fire left orientation engine\n",
    "    - 2: fire main engine\n",
    "    - 3: fire right orientation engine\n",
    "\n",
    "* Sensors: Each observation is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "Gymnasim environments are implemented as classes with a `make` method to create the environment, a `reset` method, and a `step` method to execute an action.\n",
    "To use it with an agent function that expects percetps and returns an action, we need write glue code that connects the environment with the agent function."
   ]
  },
  {
   "cell_type": "code",
   "id": "0366e4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T05:50:19.304741Z",
     "start_time": "2025-10-02T05:50:19.098576Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def run_episode(agent_function, max_steps=1000):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation (use seed=42 in reset to get reproducible results)\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode\n",
    "    for _ in range(max_steps):\n",
    "        # call the agent function to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        print (f\"Obs: {observation} -> Action: {action}\")\n",
    "\n",
    "        # step: execute an action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "        env.render()\n",
    "\n",
    "        if terminated:\n",
    "            print(f\"Final Reward: {reward}\")\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return reward"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1458d0d0",
   "metadata": {},
   "source": [
    "Note: `env.render()` shows the environment when the notebook is locally run (e.g., in VScode). On Colab, you cannot see the environment because the code is run on a headless server (i.e., a server without a display). There are some workarounds you can google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a877b57",
   "metadata": {},
   "source": [
    "## Example: A Random Agent\n",
    "\n",
    "We ranomly return one of the actions. The environment accepts the integers 0-3.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a006895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:13:32.643981Z",
     "start_time": "2025-09-29T03:13:32.622713Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_agent_function(observation): \n",
    "    \"\"\"A random agent that selects actions uniformly at random. It ignores the observation.\"\"\"\n",
    "    return np.random.choice([0, 1, 2, 3], p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "run_episode(random_agent_function)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: [-0.00191422  1.4000576  -0.19390155 -0.48278096  0.00222485  0.04392162\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.00391626  1.390079   -0.20199127 -0.44349682  0.00398342  0.0351756\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.00598555  1.3795073  -0.21041162 -0.46986824  0.00742835  0.06890505\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.00822887  1.3696134  -0.22700128 -0.43973824  0.01006807  0.0527994\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.01038752  1.3591299  -0.21637464 -0.46594182  0.0105744   0.01012736\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.01262779  1.3480498  -0.22661981 -0.4924714   0.01313464  0.05120957\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.01486816  1.3363696  -0.22662804 -0.5191399   0.01569312  0.05117427\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.0171772   1.3240843  -0.23522858 -0.546066    0.01997583  0.08566215\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.01941042  1.3122385  -0.22807793 -0.52654964  0.02468403  0.09417263\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.02164373  1.2997929  -0.22809151 -0.55322856  0.02939154  0.09415898\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.02379379  1.2867563  -0.2176464  -0.5794605   0.03199798  0.05213367\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.02588425  1.2731308  -0.21014924 -0.60560066  0.03309519  0.02194613\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.02790184  1.2589128  -0.20100124 -0.6318891   0.0323544  -0.01481728\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.0300148   1.2448618  -0.21017429 -0.6244681   0.0312416  -0.02225795\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.03212776  1.2302105  -0.21017078 -0.6511431   0.03012893 -0.02225547\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.03429852  1.2158062  -0.21575978 -0.64016545  0.0288241  -0.02609892\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.03655405  1.2007958  -0.22638574 -0.66714495  0.02965226  0.01656474\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.03880959  1.1851854  -0.22638893 -0.6938101   0.03047932  0.01654257\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.0411603   1.1695981  -0.23552302 -0.6927795   0.03092036  0.00882144\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.04341612  1.1534228  -0.223609   -0.7188601   0.02896868 -0.03903687\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.04573231  1.1366367  -0.23118143 -0.74603885  0.02854143 -0.00854585\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.048106    1.1203281  -0.2367713  -0.7248163   0.02793995 -0.01203041\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.05060215  1.1045357  -0.24852936 -0.7018661   0.02686671 -0.02146683\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.05319061  1.089456   -0.2574314  -0.67018527  0.02547465 -0.0278438\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.05565939  1.0745943  -0.24609534 -0.6605101   0.02470116 -0.01547113\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.05800714  1.0601788  -0.23462923 -0.6406823   0.02456691 -0.00268527\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.06035481  1.0451633  -0.23462923 -0.667349    0.02443265 -0.00268534\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.06262998  1.0295451  -0.22554381 -0.6941167   0.0224804  -0.0390453\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.06499147  1.0133288  -0.23636779 -0.7207208   0.02269471  0.00428619\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.06744862  0.9965192  -0.24835679 -0.74714243  0.02530725  0.05225083\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.06984501  0.9791053  -0.24074373 -0.77396995  0.0263972   0.02179911\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.07216702  0.9610993  -0.23140597 -0.80025256  0.02561394 -0.01566522\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.07440004  0.9424975  -0.22023737 -0.82669294  0.02259257 -0.06042783\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.07663307  0.9232959  -0.22023737 -0.85336035  0.01957118 -0.06042783\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.0788661   0.9034945  -0.22023733 -0.8800278   0.01654978 -0.06042779\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.08109913  0.88309306 -0.22023733 -0.9066952   0.01352841 -0.06042771\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.08341646  0.8620856  -0.23083174 -0.9336631   0.01263006 -0.01796689\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.08573389  0.840478   -0.23083174 -0.9603298   0.0117317  -0.01796699\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.0882205   0.8197927  -0.24701345 -0.91933376  0.01010558 -0.03252231\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.09070702  0.7985075  -0.24701342 -0.94600064  0.00847947 -0.03252231\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.09319363  0.77662224 -0.24701345 -0.9726675   0.00685335 -0.03252228\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.0957654   0.7541382  -0.25770614 -0.99929553  0.00736846  0.01030208\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.09842453  0.73216796 -0.26604772 -0.9764537   0.00750513  0.00273335\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.1009964   0.7095868  -0.25510758 -1.0035981   0.00545229 -0.04105651\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.10356827  0.6864056  -0.25510755 -1.0302651   0.00339947 -0.0410565\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-1.0622005e-01  6.6261649e-01 -2.6514390e-01 -1.0572953e+00\n",
      "  3.3570288e-03 -8.4868155e-04  0.0000000e+00  0.0000000e+00] -> Action: 2\n",
      "Obs: [-1.08866975e-01  6.39499128e-01 -2.64678895e-01 -1.02743697e+00\n",
      "  3.35077103e-03 -1.25261009e-04  0.00000000e+00  0.00000000e+00] -> Action: 2\n",
      "Obs: [-0.11142311  0.61656374 -0.25604528 -1.0193521   0.00377184  0.00842148\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.11388226  0.5940348  -0.24680515 -1.0012952   0.00465347  0.0176326\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.11624642  0.57090414 -0.23490424 -1.0280226   0.0031519  -0.03003118\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-1.18545435e-01  5.47164142e-01 -2.26714417e-01 -1.05510783e+00\n",
      "  1.07290716e-05 -6.28234074e-02  0.00000000e+00  0.00000000e+00] -> Action: 3\n",
      "Obs: [-0.12075329  0.52282655 -0.21528518 -1.0816839  -0.00541923 -0.10859893\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.12296104  0.49788925 -0.21528521 -1.1083529  -0.01084917 -0.10859872\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.12509803  0.47235203 -0.2063843  -1.1350585  -0.01806168 -0.14425024\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.12734804  0.44708872 -0.21712057 -1.1229291  -0.0258463  -0.15569267\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.12966251  0.42122176 -0.22519645 -1.1497612  -0.03201575 -0.12338887\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.13197699  0.39475536 -0.22519657 -1.1764311  -0.03818516 -0.12338857\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.13429146  0.36768946 -0.22519675 -1.2031008  -0.04435457 -0.12338823\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.13653985  0.34002808 -0.21692112 -1.2296513  -0.05217766 -0.15646176\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.13887778  0.31176582 -0.22814277 -1.2563072  -0.05775627 -0.11157218\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.14121561  0.2829039  -0.228143   -1.2829765  -0.06333487 -0.11157195\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.14345817  0.25344574 -0.21620798 -1.3096147  -0.07129806 -0.15926386\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.14580135  0.22411495 -0.2256304  -1.3040253  -0.07988474 -0.17173378\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.14824018  0.19418694 -0.2376478  -1.3304808  -0.08606604 -0.12362619\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.15059166  0.16365725 -0.22670206 -1.3573823  -0.09443881 -0.16745527\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.15285234  0.13251658 -0.21531323 -1.3847438  -0.10510686 -0.21336095\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.15503912  0.1007553  -0.20603645 -1.4125521  -0.11766767 -0.25121593\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.15730496  0.06841944 -0.21599571 -1.438018   -0.12819411 -0.21052888\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.15930548  0.03657554 -0.19028047 -1.4161512  -0.13793465 -0.1948109\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.16110973  0.00486772 -0.17124099 -1.4101136  -0.1470874  -0.18305486\n",
      "  1.          1.        ] -> Action: 0\n",
      "Obs: [-0.16262302 -0.025577   -0.14067447 -1.3540317  -0.15562062 -0.16805857\n",
      "  1.          1.        ] -> Action: 3\n",
      "Final Reward: -100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "663236fb",
   "metadata": {},
   "source": [
    "## A Simple Reflex-Based Agent\n",
    "\n",
    "To make the code easier to read, we use enumerations for actions (integers) and observations (index in the observation vector)."
   ]
  },
  {
   "cell_type": "code",
   "id": "add54072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T05:49:38.160474Z",
     "start_time": "2025-10-02T05:49:38.152938Z"
    }
   },
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Act(Enum):\n",
    "    LEFT = 1\n",
    "    RIGHT = 3\n",
    "    MAIN = 2\n",
    "    NO_OP = 0\n",
    "\n",
    "class Obs(Enum):\n",
    "    X = 0\n",
    "    Y = 1\n",
    "    VX = 2\n",
    "    VY = 3\n",
    "    ANGLE = 4\n",
    "    ANGULAR_VELOCITY = 5\n",
    "    LEFT_LEG_CONTACT = 6\n",
    "    RIGHT_LEG_CONTACT = 7\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Implement A Better Reflex-Based Agent\n",
    "\n",
    "Build a better that uses its right and left thrusters to land the craft (more) safely. Test your agent function using 100 problems."
   ],
   "id": "b272c703cfb907c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T05:49:42.074206Z",
     "start_time": "2025-10-02T05:49:42.007316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rocket_agent_function(observation):\n",
    "    \"\"\"Rule-based agent for lunar lander.\"\"\"\n",
    "\n",
    "    rules = [\n",
    "        # (điều kiện, hành động)\n",
    "        #Hãm tốc độ rơi\n",
    "        (lambda obs: obs[Obs.VY.value] < -0.2, Act.MAIN.value),\n",
    "        #Canh góc\n",
    "        (lambda obs: obs[Obs.ANGLE.value] > 0.15, Act.RIGHT.value), #mui tau nghieng phai nen dung dong co phai tra day ve\n",
    "        (lambda obs: obs[Obs.ANGLE.value] < -0.15, Act.LEFT.value), #mui tau nghieng trai nen dung dong co trai tra day ve\n",
    "        #Đưa tàu về bãi đáp chỉ định\n",
    "        (lambda obs: obs[Obs.X.value] > 0.1, Act.LEFT.value),\n",
    "        (lambda obs: obs[Obs.X.value] < -0.1, Act.RIGHT.value)\n",
    "    ]\n",
    "\n",
    "    # chạy qua từng rule\n",
    "    for condition, action in rules:\n",
    "        if condition(observation):\n",
    "            return action\n",
    "\n",
    "    return Act.NO_OP.value\n",
    "run_episode(rocket_agent_function)\n"
   ],
   "id": "7ae820d27c2519f1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     30\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m action\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Act.NO_OP.value\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m \u001B[43mrun_episode\u001B[49m(rocket_agent_function)\n",
      "\u001B[31mNameError\u001B[39m: name 'run_episode' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating the Agent\n",
    "\n",
    "Run the agent on 100 problems and report the average reward."
   ],
   "id": "c7868d743319aa46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:13:33.360307Z",
     "start_time": "2025-09-29T03:13:32.712302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 1: VY < -0.3\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "15ff801473b214f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, -100, 100, -100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Average reward: -82.0\n",
      "Success rate: 9/100\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:31:11.255498Z",
     "start_time": "2025-09-29T03:31:09.932820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 2: VY < -0.2\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "1b981f83d91d19e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, -100, -100, -100, -100, 100, -100, -100, np.float64(-0.0806241681046842), 100, 100, -100, 100, -100, -100, 100, -100, 100, -100, 100, 100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, 100, -100, 100, 100, -100, -100, -100, -100, 100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, 100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, 100, -100, -100, 100, -100, 100, 100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, 100, np.float64(1.234205915060316), -100, -100, 100, 100]\n",
      "Average reward: -29.988464182530443\n",
      "Success rate: 34/100\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:31:05.650307Z",
     "start_time": "2025-09-29T03:31:04.438895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.15\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "4efc63ac214ad042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, 100, -100, 100, 100, 100, -100, -100, -100, -100, -100, 100, -100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, -100, np.float64(-0.22023092004587738), -100, 100, -100, np.float64(-0.021794024895214648), 100, 100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, 100, 100, -100, -100, 100, -100, -100, -100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, 100]\n",
      "Average reward: -50.00242024944941\n",
      "Success rate: 24/100\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.2 | -0.1<X & X>0.1 left right\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "65e379da11c44cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tàu đã hạ cánh tốt hơn và ttimf ve dich tot hon. Toi nen them rule khi chua o X bang 0 thi ko tat dong co day.",
   "id": "1f81bf2c96250520"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.2 | -0.1<X & X>0.1 left right| abs(X) > 0.1 main\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "c76ffd933b0d4c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Khong dap dat duoc hahaha, gan dat thi no bat len",
   "id": "3ffdad89da4acff9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: đổi thứ tự sang đưa về pad 0 -> canh angle -> use main\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "357bf604b45f3769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tau rớt liên tù tì =))))",
   "id": "724b8a0c4ce7f437"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:16:34.582571Z",
     "start_time": "2025-09-30T09:16:33.125543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: đổi thứ tự sang đưa về -> use main -> pad 0 -> canh angle\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "5866a0465f9f6705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Average reward: -94.0\n",
      "Success rate: 3/100\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Như hạch",
   "id": "4cf5a10b51e70f41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:43:20.646105Z",
     "start_time": "2025-09-30T09:43:19.304749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: tắt v angle (roi nhanh hon do tan suat bat main bi giam) ket qua cao nhat len toi 41/100 du tau mat can bang nhieu hơn\n",
    "#toi se thu dieu chinh angle ve 0.15 cho ra ket qua tot hon rat nhieu\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "3adcda7fa9a0fc8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, -100, -100, 100, 100, -100, 100, 100, 100, 100, 100, 100, 100, 100, -100, 100, 100, -100, -100, -100, 100, 100, 100, 100, 100, -100, 100, -100, 100, -100, 100, 100, 100, 100, -100, 100, -100, -100, 100, 100, -100, -100, 100, 100, -100, 100, -100, 100, 100, -100, 100, -100, -100, 100, 100, 100, 100, 100, -100, -100, 100, 100, 100, 100, -100, 100, 100, 100, -100, 100, 100, 100, -100, 100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, 100, -100, 100, 100, 100, -100, 100, -100, -100, -100, 100, 100]\n",
      "Average reward: 20.0\n",
      "Success rate: 60/100\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: tắt v angle (roi nhanh hon do tan suat bat main bi giam) ket qua cao nhat len toi 41/100 du tau mat can bang nhieu hơn\n",
    "#toi se thu dieu chinh angle ve 0.15 cho ra ket qua tot hon rat nhieu\n",
    "#toi them lai dieu kien v angle nhung de sau dieu kien angle\n",
    "#toi them dieu kien Y < -0.1 thi chay main ->> khong on lam\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "368d8b57076e65ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:52:38.326879Z",
     "start_time": "2025-09-30T10:52:36.447659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "d42ba1d32ad3c174",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, -100, -100, -100, 100, -100, -100, -100, 100, -100, 100, 100, 100, 100, -100, -100, 100, -100, 100, -100, -100, 100, -100, 100, -100, 100, 100, -100, -100, 100, 100, 100, -100, 100, -100, 100, np.float64(0.21965917964532025), 100, -100, -100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, -100, -100, 100, -100, 100, 100, 100, -100, 100, -100, 100, 100, 100, 100, -100, 100, 100, 100, 100, -100, 100, -100, 100, -100, 100, -100, 100, 100, 100, np.float64(0.10289941503260039), 100, -100, 100, 100, 100, -100, 100, 100, 100, -100, 100, 100, 100, 100, -100, -100, 100]\n",
      "Average reward: 28.00322558594678\n",
      "Success rate: 63/100\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T05:52:17.621354Z",
     "start_time": "2025-10-02T05:50:32.642022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "68e6b06319e52d6b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 30\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\u001B[39;00m\n\u001B[32m     28\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [run_episode_test(agent_function) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n)]\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m rewards = \u001B[43mrun_episodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrocket_agent_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;28mprint\u001B[39m(rewards)\n\u001B[32m     33\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAverage reward: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp.average(rewards)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 28\u001B[39m, in \u001B[36mrun_episodes\u001B[39m\u001B[34m(agent_function, n)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_episodes\u001B[39m(agent_function, n=\u001B[32m100\u001B[39m):\n\u001B[32m     27\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mrun_episode_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent_function\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n)]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mrun_episode_test\u001B[39m\u001B[34m(agent_function)\u001B[39m\n\u001B[32m     15\u001B[39m action = agent_function(observation)\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# step (transition) through the environment with the action\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m observation, reward, terminated, truncated, info = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m terminated:\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:665\u001B[39m, in \u001B[36mLunarLander.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    662\u001B[39m     reward = +\u001B[32m100\u001B[39m\n\u001B[32m    664\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m665\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    666\u001B[39m \u001B[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001B[39;00m\n\u001B[32m    667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.array(state, dtype=np.float32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Github\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:778\u001B[39m, in \u001B[36mLunarLander.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    776\u001B[39m     \u001B[38;5;28mself\u001B[39m.screen.blit(\u001B[38;5;28mself\u001B[39m.surf, (\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m))\n\u001B[32m    777\u001B[39m     pygame.event.pump()\n\u001B[32m--> \u001B[39m\u001B[32m778\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrender_fps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    779\u001B[39m     pygame.display.flip()\n\u001B[32m    780\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mrgb_array\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
