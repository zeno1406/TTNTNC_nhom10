{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3545c3c6",
   "metadata": {},
   "source": [
    "# Create a Simple Reflex-Based Lunar Lander Agent\n",
    "\n",
    "In this example, we will use Gymnasium, an environment to train agents via reinforcement learning (RL). We will not use RL here but just use the environment with a custom simple reflex-based agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975e747",
   "metadata": {},
   "source": [
    "## Install Gymnasium\n",
    "\n",
    "The documentation for Gymnasium is available at https://gymnasium.farama.org/ \n",
    "\n",
    "Steps:\n",
    "1. Create a new folder and open it with VS Code and install all needed Python Extensions in VS Code.\n",
    "2. Create a new virtual environment (CTRL-Shift P Python Create Environment...)\n",
    "3. I needed to install swig and the Python C++ headers on WSL2 via the terminal\n",
    "    * `sudo apt install swig`\n",
    "    * `sudo apt-get install python3-dev` \n",
    "4. Install gymnasium with the needed extras"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d0bf8a9",
   "metadata": {},
   "source": [
    "%pip install -q swig\n",
    "%pip install -q gymnasium[box2d,classic_control]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a6c9dae",
   "metadata": {},
   "source": [
    "## The Lunar Lander Environment \n",
    "\n",
    "The documentation of the environment is available at: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "* Performance Measure: A reward of -100 or +100 points for crashing or landing safely respectively. We do not use \n",
    "  intermediate rewards here.\n",
    "\n",
    "* Environment: This environment is a classic rocket trajectory optimization problem. A ship needs to land safely. The space is **continuous** with\n",
    "  x and y coordinates in the range [-2.5, 2.5]. The landing pad is at coordinate (0,0).\n",
    "\n",
    "* Actuators:  According to Pontryagin’s\n",
    "  maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off. There are four discrete actions available:\n",
    "\n",
    "    - 0: do nothing\n",
    "    - 1: fire left orientation engine\n",
    "    - 2: fire main engine\n",
    "    - 3: fire right orientation engine\n",
    "\n",
    "* Sensors: Each observation is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "Gymnasim environments are implemented as classes with a `make` method to create the environment, a `reset` method, and a `step` method to execute an action.\n",
    "To use it with an agent function that expects percetps and returns an action, we need write glue code that connects the environment with the agent function."
   ]
  },
  {
   "cell_type": "code",
   "id": "0366e4b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:26:10.493469Z",
     "start_time": "2025-10-05T10:26:10.489436Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def run_episode(agent_function, max_steps=1000):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation (use seed=42 in reset to get reproducible results)\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode\n",
    "    for _ in range(max_steps):\n",
    "        # call the agent function to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        print (f\"Obs: {observation} -> Action: {action}\")\n",
    "\n",
    "        # step: execute an action in the environment\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "        env.render()\n",
    "\n",
    "        if terminated:\n",
    "            print(f\"Final Reward: {reward}\")\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return reward"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "1458d0d0",
   "metadata": {},
   "source": [
    "Note: `env.render()` shows the environment when the notebook is locally run (e.g., in VScode). On Colab, you cannot see the environment because the code is run on a headless server (i.e., a server without a display). There are some workarounds you can google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a877b57",
   "metadata": {},
   "source": [
    "## Example: A Random Agent\n",
    "\n",
    "We ranomly return one of the actions. The environment accepts the integers 0-3.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a006895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:19:12.220507Z",
     "start_time": "2025-10-05T10:19:09.416773Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_agent_function(observation): \n",
    "    \"\"\"A random agent that selects actions uniformly at random. It ignores the observation.\"\"\"\n",
    "    return np.random.choice([0, 1, 2, 3], p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "run_episode(random_agent_function)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: [-0.00768595  1.3999902  -0.77851564 -0.48579895  0.00891286  0.17634556\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.01546936  1.3884788  -0.7896111  -0.5117229   0.02006378  0.22304049\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.02325306  1.3763682  -0.78964573 -0.53843474  0.03120808  0.22290668\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.03112469  1.3643225  -0.79806393 -0.5356246   0.04199808  0.21582015\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.0389966   1.3516784  -0.79809546 -0.562306    0.05278661  0.21579078\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.04678421  1.338435   -0.78750765 -0.58892643  0.06144578  0.17319937\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.05447731  1.3245988  -0.77564585 -0.6152183   0.0677167   0.12543003\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.06228695  1.3110851  -0.78693426 -0.6008979   0.07363201  0.11831703\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.07009687  1.2969716  -0.7869502  -0.62757677  0.07954618  0.11829418\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.07783089  1.2822785  -0.77739614 -0.65323436  0.08351599  0.07940304\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.08549194  1.2669885  -0.76827097 -0.6796804   0.08565392  0.04276278\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.09315319  1.2510986  -0.76827586 -0.7063495   0.08779191  0.04276358\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.10075025  1.2359335  -0.7625336  -0.6741649   0.09059662  0.05609956\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.10853796  1.2215619  -0.7810924  -0.6388852   0.09290056  0.04608304\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.11632566  1.20659    -0.78109753 -0.6655603   0.09520489  0.04609073\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.12418976  1.1910124  -0.7906586  -0.69261676  0.09942881  0.08448587\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.13223247  1.1753477  -0.8078353  -0.6964496   0.10297649  0.07095996\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.14042445  1.1604518  -0.8224966  -0.6622733   0.10626805  0.06583669\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.1486164   1.1449558  -0.8225045  -0.6889526   0.10955907  0.06582607\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.15680857  1.1288598  -0.8225144  -0.7156189   0.11284947  0.06581379\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.16492434  1.1121784  -0.8129277  -0.7414989   0.11419521  0.02691681\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.17309074  1.096171   -0.81821215 -0.7115612   0.11575684  0.03123555\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.18118982  1.0795736  -0.8097684  -0.73764575  0.11561079 -0.0029211\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.189289    1.0623765  -0.8097665  -0.76430935  0.11546554 -0.0029053\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.19738808  1.0445793  -0.8097661  -0.79097676  0.11531997 -0.00291186\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.20547995  1.0261724  -0.80944365 -0.818099    0.11557667  0.00513173\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.21364117  1.0071437  -0.8176054  -0.8458398   0.11704965  0.02945945\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.22173333  0.987529   -0.808931   -0.8717398   0.1167635  -0.00572274\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.22982554  0.9673143  -0.80893105 -0.89840645  0.11647736 -0.00572282\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.23782983  0.9465265  -0.79786646 -0.9237033   0.11393019 -0.05094345\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.24601293  0.9265167  -0.8154038  -0.8891121   0.1110449  -0.05770578\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.25419608  0.90590674 -0.8154036  -0.9157794   0.10815962 -0.05770585\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.26228362  0.8847231  -0.8033845  -0.94111556  0.10282709 -0.10665065\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.27042967  0.8636055  -0.80917025 -0.9382007   0.09743648 -0.10781224\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.27859443  0.8434417  -0.8114255  -0.89584583  0.09242567 -0.10021597\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.28675923  0.82267827 -0.8114252  -0.9225146   0.08741488 -0.10021595\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.2948324   0.8013205  -0.799932   -0.94882303  0.08009952 -0.14630738\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.30297822  0.77935714 -0.8090455  -0.9758605   0.07461454 -0.10969949\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.3110538   0.7567984  -0.8002292  -1.0022662   0.06736161 -0.14505868\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.3191958   0.7336282  -0.80856216 -1.0295388   0.06178914 -0.11144935\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.32725912  0.7113911  -0.80135775 -0.9881209   0.05687144 -0.09835386\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.33547863  0.68932915 -0.81638736 -0.9803333   0.05136873 -0.11005435\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.34356755  0.6673664  -0.80404556 -0.9759622   0.04657972 -0.09578038\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.35165653  0.64480394 -0.8040455  -1.0026307   0.04179069 -0.09578077\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.3598153   0.62163323 -0.8128123  -1.0297333   0.03876266 -0.06056059\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.3679141   0.5978687  -0.805285   -1.0560862   0.03422375 -0.09077811\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.37591568  0.57350487 -0.7930854  -1.0826911   0.02724242 -0.13962668\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.38391724  0.5485417  -0.7930852  -1.1093618   0.02026111 -0.13962618\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.3918348   0.52297646 -0.782545   -1.1361369   0.01117032 -0.181816\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.39992028  0.49754107 -0.79857993 -1.1304168   0.00132853 -0.19683568\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.40808457  0.47204855 -0.8060945  -1.1330178  -0.00887515 -0.2040736\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.41615683  0.44595417 -0.7945434  -1.1598727  -0.02139253 -0.2503476\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.424229    0.4192619  -0.7945436  -1.1865525  -0.03390978 -0.25034493\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.43239003  0.39196432 -0.8056815  -1.2134917  -0.0442012  -0.2058284\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.44045204  0.36406827 -0.79328233 -1.2402527  -0.05697468 -0.25546974\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.4484362   0.33557326 -0.7835253  -1.2670759  -0.07170204 -0.29454738\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.45650825  0.30648673 -0.7945753  -1.2933856  -0.08421224 -0.25020397\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.46458024  0.27680227 -0.7945768  -1.3200654  -0.0967223  -0.25020128\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.4726521   0.24651998 -0.7945787  -1.3467453  -0.10923223 -0.25019854\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.48063007  0.21561907 -0.78280246 -1.3745326  -0.12413234 -0.29800218\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.48869142  0.18414296 -0.79332507 -1.4000542  -0.13688731 -0.25509956\n",
      "  0.          1.        ] -> Action: 2\n",
      "Obs: [-0.49627858  0.1537624  -0.72282577 -1.353563   -0.16675773 -0.58999366\n",
      "  0.          1.        ] -> Action: 2\n",
      "Final Reward: -100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "663236fb",
   "metadata": {},
   "source": [
    "## A Simple Reflex-Based Agent\n",
    "\n",
    "To make the code easier to read, we use enumerations for actions (integers) and observations (index in the observation vector)."
   ]
  },
  {
   "cell_type": "code",
   "id": "add54072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:16:05.054437Z",
     "start_time": "2025-10-05T10:16:05.050456Z"
    }
   },
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Act(Enum):\n",
    "    LEFT = 1\n",
    "    RIGHT = 3\n",
    "    MAIN = 2\n",
    "    NO_OP = 0\n",
    "\n",
    "class Obs(Enum):\n",
    "    X = 0\n",
    "    Y = 1\n",
    "    VX = 2\n",
    "    VY = 3\n",
    "    ANGLE = 4\n",
    "    ANGULAR_VELOCITY = 5\n",
    "    LEFT_LEG_CONTACT = 6\n",
    "    RIGHT_LEG_CONTACT = 7\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Implement A Better Reflex-Based Agent\n",
    "\n",
    "Build a better that uses its right and left thrusters to land the craft (more) safely. Test your agent function using 100 problems."
   ],
   "id": "b272c703cfb907c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:16:20.700289Z",
     "start_time": "2025-10-05T10:16:07.452207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rocket_agent_function(observation):\n",
    "    \"\"\"Rule-based agent for lunar lander.\"\"\"\n",
    "\n",
    "    rules = [\n",
    "        # (điều kiện, hành động)\n",
    "        #Hãm tốc độ rơi\n",
    "        (lambda obs: obs[Obs.VY.value] < -0.2, Act.MAIN.value),\n",
    "        #Canh góc\n",
    "        (lambda obs: obs[Obs.ANGLE.value] > 0.15, Act.RIGHT.value), #mui tau nghieng phai nen dung dong co phai tra day ve\n",
    "        (lambda obs: obs[Obs.ANGLE.value] < -0.15, Act.LEFT.value), #mui tau nghieng trai nen dung dong co trai tra day ve\n",
    "        #Đưa tàu về bãi đáp chỉ định\n",
    "        (lambda obs: obs[Obs.X.value] > 0.1, Act.LEFT.value),\n",
    "        (lambda obs: obs[Obs.X.value] < -0.1, Act.RIGHT.value)\n",
    "    ]\n",
    "\n",
    "    # chạy qua từng rule\n",
    "    for condition, action in rules:\n",
    "        if condition(observation):\n",
    "            return action\n",
    "\n",
    "    return Act.NO_OP.value\n",
    "run_episode(rocket_agent_function)\n"
   ],
   "id": "7ae820d27c2519f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs: [ 0.00534973  1.4050221   0.5418712  -0.26213983 -0.00619238 -0.12274197\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.01085701  1.399317    0.55611044 -0.2535941  -0.01151624 -0.10648684\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.01637573  1.3937044   0.5572207  -0.24949823 -0.01680839 -0.10585276\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.0218297   1.3882722   0.55110806 -0.24150516 -0.02245212 -0.11288514\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.02733202  1.3835702   0.55580264 -0.20907079 -0.0279589  -0.11014573\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.03301163  1.3795296   0.5727747  -0.17968592 -0.03273122 -0.09545504\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.03869123  1.3748889   0.5727879  -0.20636104 -0.0375033  -0.09545038\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.0443901   1.3702224   0.57468474 -0.20753294 -0.04225169 -0.09497631\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.05022135  1.3661904   0.5874549  -0.17933106 -0.04653551 -0.0856843\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.05605278  1.3615584   0.5874666  -0.20600878 -0.05081901 -0.08567782\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.06188173  1.3574586   0.587417   -0.1823754  -0.05529506 -0.08952908\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.06771078  1.3527588   0.58742917 -0.20905475 -0.05977022 -0.08951119\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.07358436  1.3482769   0.5918092  -0.19938235 -0.06418961 -0.08839594\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.07945804  1.3431951   0.59182125 -0.22605935 -0.06860835 -0.08838278\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.08554535  1.3384607   0.6123843  -0.21058787 -0.0722261  -0.07236132\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.09150781  1.3342773   0.6007504  -0.18615113 -0.07668996 -0.08928504\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.09747057  1.3294939   0.6007627  -0.21282707 -0.08115204 -0.08924947\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.10355816  1.3246663   0.6128096  -0.21479382 -0.08517098 -0.08038609\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.10977173  1.3202125   0.62506413 -0.19816239 -0.0888612  -0.07381088\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.11589555  1.3151698   0.6138089  -0.22420095 -0.09028154 -0.02840885\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.12208176  1.3110696   0.6202036  -0.18232134 -0.0918437  -0.03124628\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.12818928  1.3063815   0.61031324 -0.2083378  -0.0914083   0.00870867\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.1345067   1.3023357   0.6306776  -0.17974335 -0.09035262  0.02111547\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.14075632  1.2977011   0.6221766  -0.20582087 -0.08758173  0.05542283\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.14706469  1.2935076   0.6280495  -0.1862212  -0.08482131  0.05521345\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.15330382  1.2887015   0.61939245 -0.21334523 -0.08034382  0.08954988\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.15963069  1.2845035   0.6280528  -0.1863425  -0.07576013  0.09167375\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.16587067  1.2797107   0.617157   -0.21268068 -0.06899131  0.13537654\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.1720314   1.2757493   0.6099081  -0.17579314 -0.06289143  0.12199745\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.17810202  1.2712077   0.59858286 -0.20151296 -0.05450543  0.16772012\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.18428794  1.2674518   0.609811   -0.16663736 -0.04581843  0.1737397\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.19037704  1.2630985   0.59764606 -0.19318552 -0.03469581  0.2224526\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.19638424  1.2581613   0.587371   -0.21916859 -0.02150777  0.26376075\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.20256558  1.2534153   0.6040084  -0.21078952 -0.00756697  0.2788163\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.20867653  1.2495446   0.5973296  -0.1720164   0.00601447  0.27162904\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.21470118  1.2450681   0.58651024 -0.1990888   0.02176332  0.3149772\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.22063112  1.239991    0.5746454  -0.22601482  0.03988948  0.36252332\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2265273   1.2350312   0.57134986 -0.2210189   0.05794785  0.3611675\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.23250046  1.2301592   0.5785708  -0.21736287  0.07648401  0.3707233\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.23848876  1.2258054   0.5797908  -0.19457918  0.09535465  0.37741286\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.24439554  1.2208452   0.5695883  -0.22192432  0.11628284  0.4185636\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2501622   1.2164268   0.55587506 -0.19812135  0.13697648  0.41387278\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.2558481   1.2113926   0.5457772  -0.2259957   0.1597347   0.45516405\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.26138526  1.2071278   0.53098387 -0.19214754  0.18248495  0.4550055\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.26700306  1.2022963   0.54127604 -0.21737957  0.20310824  0.412466\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.27255648  1.1978941   0.5345849  -0.19864081  0.22408037  0.41944256\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.27820128  1.1929241   0.54617053 -0.22378317  0.242661    0.3716126\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.28377008  1.1888736   0.5378755  -0.18327524  0.2620265   0.38730997\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.28941995  1.1842716   0.54826415 -0.2076188   0.27915365  0.34254372\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.29462186  1.1805074   0.50428045 -0.17041375  0.29551083  0.32714412\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.2998867   1.1761744   0.5123175  -0.19552164  0.31015593  0.29290175\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.30523816  1.1712786   0.52328163 -0.22017387  0.32247722  0.24642627\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3104724   1.1670995   0.5108732  -0.18859057  0.33557203  0.26189584\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.31578618  1.1623591   0.52094585 -0.21314517  0.34649894  0.21853793\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3207387   1.1580375   0.48534828 -0.19449247  0.3569221   0.20846243\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3257784   1.1531565   0.49632764 -0.21885687  0.36498097  0.1611784\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.33053723  1.1489754   0.46806446 -0.18784139  0.37325826  0.16554609\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3353818   1.1442462   0.47894615 -0.21163662  0.37911057  0.1170463\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.34004134  1.139421    0.4606556  -0.21587303  0.384743    0.11264773\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.34456673  1.134947    0.4467671  -0.2004171   0.3909022   0.12318349\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.34905928  1.1307025   0.44271797 -0.19045745  0.39789608  0.13987733\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.35363054  1.1258974   0.45261407 -0.21482888  0.4027271   0.09662007\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.35791206  1.1215563   0.42360488 -0.19423598  0.40761206  0.09769937\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3622755   1.1166697   0.43401223 -0.21785596  0.41012716  0.05030201\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.36627513  1.1121695   0.39792985 -0.20059918  0.4123121   0.04369833\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.37000027  1.1080687   0.37042567 -0.18286164  0.41457018  0.04516212\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.37379655  1.1034198   0.37946585 -0.20666997  0.4147361   0.00331845\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 3.7726593e-01  1.0991534e+00  3.4693325e-01 -1.8962009e-01\n",
      "  4.1474018e-01  8.1733553e-05  0.0000000e+00  0.0000000e+00] -> Action: 3\n",
      "Obs: [ 0.38078755  1.0943307   0.35363516 -0.2139111   0.41315395 -0.03172437\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.38396662  1.0899276   0.31955984 -0.19521156  0.41138    -0.03547987\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3872339   1.0849857   0.33076844 -0.21847658  0.40703458 -0.08690809\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.38999653  1.0805267   0.2810657  -0.19680355  0.40186822 -0.10332757\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3928174   1.0754943   0.2883895  -0.22188507  0.39511803 -0.1350041\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.39534777  1.0704089   0.25991142 -0.2241291   0.3877771  -0.14681855\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3976857   1.065505    0.24059789 -0.21611756  0.38052362 -0.14506924\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.39997473  1.0608089   0.23506315 -0.20708992  0.37398642 -0.13074484\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40199813  1.0559597   0.20911379 -0.21376719  0.36680892 -0.1435498\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40388498  1.0513597   0.19515078 -0.20280987  0.35998175 -0.13654323\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4054862   1.0465707   0.16736151 -0.21105127  0.35236531 -0.15232843\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40676707  1.041699    0.13612476 -0.21457134  0.3439081  -0.16914374\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40797073  1.0371046   0.12788877 -0.20241746  0.3360281  -0.15760024\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4091813   1.0325676   0.12793455 -0.20006317  0.32884723 -0.14361712\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41020042  1.0284957   0.10859258 -0.17947407  0.32188976 -0.13914941\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.411277    1.0238655   0.11593509 -0.2039722   0.31327778 -0.17223933\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41221792  1.0196737   0.10202041 -0.18460552  0.30505684 -0.1644186\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.41323346  1.0149293   0.11154145 -0.20880209  0.29473656 -0.20640536\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4141225   1.0110434   0.09818867 -0.17085947  0.2851635  -0.19146128\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.41509637  1.0065982   0.10895407 -0.19534451  0.27328953 -0.23747909\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.41614038  1.0015846   0.1178337  -0.22037609  0.25953645 -0.27506158\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41691732  0.9968196   0.09170768 -0.20936902  0.2452626  -0.28547695\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4176507   0.99234897  0.0869161  -0.19649002  0.23146294 -0.2759931\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.41845816  0.9873143   0.09632985 -0.22139119  0.21567509 -0.3157571\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41899577  0.9825697   0.07004178 -0.20858337  0.19925699 -0.32836205\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41924706  0.9782394   0.04213732 -0.1902581   0.1821609  -0.341922\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.41956845  0.97333235  0.05100168 -0.21588548  0.16324788 -0.37826034\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41969928  0.9687148   0.03243108 -0.20321351  0.14391218 -0.38671416\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41966772  0.96464026  0.01649356 -0.17930724  0.12434032 -0.39143708\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41957512  0.9599683   0.00889495 -0.20622303  0.1062903  -0.36099997\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.419526    0.9562018   0.01258559 -0.16625214  0.08893442 -0.34711763\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41939753  0.95183074  0.00267154 -0.19341235  0.07357291 -0.30723017\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41918916  0.94685954 -0.00734398 -0.22033009  0.06021884 -0.26708138\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.418979    0.9426723  -0.00775962 -0.18562534  0.04711458 -0.26208526\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.4186781   0.93788856 -0.01912748 -0.21230732  0.03628473 -0.21659687\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41838065  0.93382204 -0.01893917 -0.18051434  0.02561941 -0.21330674\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41801852  0.9291543  -0.02705187 -0.20732322  0.01657968 -0.18079466\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41778794  0.92446846 -0.01455239 -0.20818606  0.00817799 -0.16803385\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 4.1746083e-01  9.2044586e-01 -2.3774009e-02 -1.7876010e-01\n",
      " -6.3682621e-04 -1.7629626e-01  0.0000000e+00  0.0000000e+00] -> Action: 1\n",
      "Obs: [ 0.41704932  0.9158206  -0.03438234 -0.20558688 -0.00732717 -0.13380668\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41666833  0.91171175 -0.03143708 -0.18266337 -0.01390024 -0.13146117\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41622382  0.9069978  -0.03941941 -0.20956491 -0.01887611 -0.09951721\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4156494   0.9022204  -0.05175439 -0.21241187 -0.02448276 -0.11213316\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41523713  0.8983688  -0.03619821 -0.17127511 -0.02945455 -0.09943593\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41474447  0.8939202  -0.04627231 -0.19777127 -0.03240784 -0.05906565\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41415614  0.88887465 -0.05827437 -0.22426048 -0.03295663 -0.01097601\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41342917  0.88473976 -0.07133125 -0.18380359 -0.03429859 -0.0268392\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41262713  0.8800166  -0.08076225 -0.20990649 -0.03374563  0.01105902\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41199368  0.87560207 -0.06459457 -0.19617984 -0.03250714  0.02476991\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.41126624  0.87058467 -0.07638799 -0.22292046 -0.02890901  0.07196222\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.41052264  0.8664093  -0.0778038  -0.18550579 -0.02551345  0.06791173\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.4097124   0.86163026 -0.08615042 -0.21232368 -0.02044835  0.10130175\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40884122  0.8572482  -0.09189879 -0.19469985 -0.01572736  0.09441989\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.40788755  0.852274   -0.10225336 -0.22101778 -0.0089309   0.13592926\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40683928  0.8472814  -0.11125454 -0.22186989 -0.00258761  0.12686586\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4058276   0.84307325 -0.1077546  -0.18703656  0.00391486  0.13004945\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.40472516  0.8382551  -0.1191414  -0.214185    0.01269838  0.17567018\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.40369922  0.8341102  -0.11190394 -0.18432902  0.02188614  0.1837552\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.40260458  0.8293582  -0.12051121 -0.21140724  0.03280056  0.21828815\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.4014186   0.82506895 -0.12932418 -0.1909092   0.04340082  0.2120051\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.40014687  0.8201858  -0.14007847 -0.21745268  0.05615054  0.2549947\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3988391   0.81583345 -0.1436992  -0.19397143  0.06893037  0.25559643\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.39746848  0.81088156 -0.15155531 -0.22081071  0.08328396  0.28707188\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.39617157  0.80691075 -0.14493114 -0.17739862  0.0983831   0.3019827\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.39479923  0.80233854 -0.15435633 -0.20442624  0.11537354  0.33980894\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3934409   0.7980014  -0.15327069 -0.19420287  0.13272569  0.34704283\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.39201593  0.793065   -0.16156796 -0.2212034   0.15174076  0.38030118\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3904108   0.78828967 -0.17906457 -0.21423143  0.17028674  0.37092033\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.38876715  0.78445643 -0.18348683 -0.17266983  0.18946348  0.3835348\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.387204    0.7800539  -0.17326948 -0.19792391  0.20652905  0.3413115\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3857294   0.7750753  -0.16209522 -0.22338675  0.22131042  0.29562762\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.38413352  0.770348   -0.17420664 -0.21235703  0.23612112  0.29621404\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.38223028  0.7662652  -0.20441453 -0.18378374  0.25044796  0.28653735\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.38040942  0.7616132  -0.19401042 -0.20883669  0.26260462  0.24313347\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3784341   0.7578813  -0.20992646 -0.16813356  0.27528942  0.25369582\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.376544    0.75359976 -0.19905438 -0.19221431  0.28561258  0.20646353\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3747221   0.74873775 -0.19051857 -0.21774198  0.29417434  0.17123495\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.37287253  0.74383235 -0.19361895 -0.21979006  0.30311197  0.17875305\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.37091914  0.7393902  -0.20448153 -0.19936933  0.31257024  0.1891656\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3690303   0.734384   -0.19626394 -0.22410798  0.32023823  0.15335995\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.36704072  0.72969705 -0.20674822 -0.21005337  0.328357    0.16237536\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.36489955  0.7254812  -0.22229083 -0.18925187  0.33687678  0.17039536\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.36284018  0.7206976  -0.2119833  -0.21402732  0.34321222  0.12670933\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.36041546  0.7163438  -0.24801524 -0.19483522  0.34902528  0.11626153\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3580742   0.7114473  -0.2373654  -0.21840814  0.35243407  0.06817509\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.355441    0.70736593 -0.2667938  -0.18224469  0.35608223  0.07296317\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.352876    0.7027322  -0.25808316 -0.20634326  0.3577569   0.03349286\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.34993258  0.6984751  -0.29537886 -0.1894572   0.35884726  0.02180712\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.34707108  0.6936695  -0.2849771  -0.21329427  0.35761163 -0.02471289\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3439811   0.68937725 -0.30802396 -0.19052759  0.3565887  -0.02045883\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.3409647   0.68451566 -0.29879978 -0.21537128  0.35359454 -0.05988304\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.33752355  0.6802361  -0.34071723 -0.18936972  0.35002065 -0.07147763\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.33417186  0.6754163  -0.32930917 -0.21280104  0.3438812  -0.12278885\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3305974   0.6709667  -0.35161772 -0.19638053  0.33778068 -0.12201004\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.32708654  0.6659526  -0.34355992 -0.22111394  0.32991552 -0.15730295\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.32339725  0.6613135  -0.36156613 -0.20452812  0.3222444  -0.1534224\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.31970826  0.65685743 -0.3622633  -0.19658098  0.3153396  -0.13809623\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.31608468  0.6518429  -0.35392752 -0.22105421  0.30659044 -0.17498319\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3123029   0.647741   -0.37040254 -0.1806882   0.29855275 -0.16075328\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.30859718  0.6430859  -0.36070985 -0.2049139   0.28838825 -0.20328978\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.3045228   0.63882107 -0.39680618 -0.18747343  0.27745852 -0.2185944\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.30054408  0.6339901  -0.38474858 -0.21227673  0.2640137  -0.26889637\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2963462   0.62962204 -0.4064899  -0.1918015   0.25044686 -0.27133775\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.2922328   0.6246862  -0.3958186  -0.21680053  0.23465337 -0.31586978\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.287782    0.6202021  -0.42873424 -0.19677715  0.2180977  -0.3311132\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.28340608  0.61514425 -0.4192465  -0.22219373  0.19958483 -0.37025753\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.27884388  0.61070955 -0.4377069  -0.19471489  0.18097663 -0.37216383\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [ 0.27435884  0.60570467 -0.4279166  -0.22005053  0.16034934 -0.41254568\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2696851   0.60085124 -0.446227   -0.21355891  0.13923147 -0.4223575\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2647787   0.5966667  -0.46889472 -0.18409176  0.11759296 -0.43277034\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.25978184  0.5918604  -0.4802422  -0.21219367  0.09827546 -0.38635024\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.25472203  0.5875356  -0.48653603 -0.1910449   0.07899675 -0.3855741\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.2495637   0.58260125 -0.49885568 -0.21849175  0.06220116 -0.3359113\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.24424744  0.5782179  -0.5141233  -0.1941828   0.04489798 -0.34606367\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.23885965  0.57324296 -0.5230686  -0.2207132   0.02938241 -0.31031165\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.233572    0.5682628  -0.5135856  -0.22110942  0.01438356 -0.29997697\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.22811213  0.56373656 -0.53003335 -0.20109354 -0.00136338 -0.3149387\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.22267513  0.5596978  -0.5278486  -0.17959012 -0.0170081  -0.31289428\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.21717492  0.5550558  -0.53579223 -0.20653455 -0.03106371 -0.28111213\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.2117608   0.55118126 -0.52744615 -0.17254879 -0.0448505  -0.27573606\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.20624971  0.5467226  -0.53964984 -0.19854307 -0.05618414 -0.22667265\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.2006733   0.5416653  -0.5478382  -0.22516163 -0.06587937 -0.19390485\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.19529752  0.53728074 -0.528447   -0.19529799 -0.07490569 -0.18052632\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.18983383  0.53229725 -0.53948706 -0.22185038 -0.08172404 -0.13636675\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.18435125  0.52773523 -0.54102784 -0.2031662  -0.08888557 -0.14323062\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.17883301  0.5231995  -0.5442618  -0.20205341 -0.09638239 -0.14993645\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.17338142  0.51924294 -0.53755945 -0.17635885 -0.10392141 -0.15078035\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.16783714  0.51469    -0.54918444 -0.20272581 -0.10913321 -0.10423647\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.16236296  0.5109665  -0.54201543 -0.16589445 -0.11450161 -0.10736784\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.15681401  0.5066537  -0.55140656 -0.1919496  -0.11797523 -0.06947215\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.15119477  0.5017484  -0.56021124 -0.21815126 -0.1196772  -0.03403965\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.14574623  0.49695805 -0.5436731  -0.2130008  -0.12085846 -0.02362525\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.14037228  0.49214196 -0.5363327  -0.21413608 -0.1219098  -0.02102651\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.13514146  0.48808172 -0.52217555 -0.18053168 -0.12281176 -0.01803964\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.12983684  0.48342484 -0.53141606 -0.20689407 -0.12186316  0.01897198\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.12448158  0.4793687  -0.5357913  -0.18025047 -0.12159605  0.00534225\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.11904383  0.47471738 -0.54611737 -0.20653345 -0.11925881  0.04674451\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.11348114  0.47011608 -0.55780476 -0.20438315 -0.11773368  0.03050261\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.10795508  0.46586573 -0.55397034 -0.18880017 -0.11638435  0.02698677\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [ 0.10236158  0.46102405 -0.5624169  -0.21494763 -0.11333216  0.06104358\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.09700756  0.456322   -0.5393231  -0.20868479 -0.1094372   0.07789904\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.09186058  0.45238954 -0.5191169  -0.17446299 -0.10505075  0.08772894\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.08671369  0.44785726 -0.5191172  -0.20113127 -0.10066431  0.08772884\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.08151789  0.44429234 -0.5232813  -0.15819426 -0.09698684  0.0735494\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.07632218  0.4401276  -0.5232815  -0.18486206 -0.09330939  0.07354936\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.07112646  0.43536302 -0.5232817  -0.21152982 -0.08963192  0.07354926\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.06585302  0.43129486 -0.5303427  -0.18063337 -0.08666977  0.05924334\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.06057949  0.42662674 -0.5303429  -0.20730077 -0.08370759  0.05924332\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.05535469  0.4221219  -0.52550423 -0.200048   -0.08071736  0.05980464\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.05022669  0.41819316 -0.5159726  -0.17444324 -0.07757208  0.06290536\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.04509868  0.41366458 -0.51597273 -0.20111077 -0.07442681  0.06290533\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.04018421  0.40935862 -0.49543113 -0.19118352 -0.07048515  0.07883326\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.03526964  0.4044529  -0.49543136 -0.21785147 -0.06654348  0.07883319\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.03054314  0.39991352 -0.4773139  -0.20154814 -0.06193203  0.09222902\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.02592602  0.39606494 -0.46664524 -0.17085026 -0.05706147  0.09741124\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.0213088   0.39161676 -0.4666454  -0.19751889 -0.05219092  0.09741111\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.01669168  0.38656884 -0.46664554 -0.22418755 -0.04732038  0.09741094\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.01224842  0.3818405  -0.44993478 -0.20998369 -0.04178005  0.11080654\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [ 0.00780573  0.37785625 -0.4497047  -0.17693475 -0.03640919  0.10741726\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [ 0.00336304  0.37327245 -0.4497049  -0.20360382 -0.03103833  0.10741706\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.00115128  0.36954024 -0.45640063 -0.16577895 -0.02613684  0.09802988\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.00566568  0.3652084  -0.4564007  -0.19244763 -0.02123535  0.09802973\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.01018     0.3602769  -0.45640078 -0.2191163  -0.01633388  0.09802956\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.01457682  0.35575455 -0.44514236 -0.20094347 -0.01093883  0.10790084\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.01879635  0.35188952 -0.42821008 -0.17174667 -0.00476594  0.12345813\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.02301598  0.34742504 -0.42821017 -0.19841653  0.00140695  0.12345781\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.02723551  0.34236103 -0.42821026 -0.22508639  0.00757982  0.12345751\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.03155537  0.337857   -0.4378036  -0.20021628  0.0133165   0.11473347\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.03593121  0.3334066  -0.443152   -0.19785693  0.01882228  0.11011568\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.04030695  0.32835653 -0.44315195 -0.22452617  0.02432806  0.11011547\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.04456577  0.32368127 -0.43207866 -0.20790564  0.03044822  0.12240318\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.04902935  0.31988776 -0.45173615 -0.16871905  0.03576201  0.10627583\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.05349293  0.31549466 -0.45173612 -0.1953881   0.0410758   0.1062756\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.05795651  0.3105019  -0.45173597 -0.22205713  0.04638957  0.1062754\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.06229343  0.30627882 -0.43986303 -0.18789689  0.05248498  0.12190811\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.06663046  0.3014562  -0.4398628  -0.21456666  0.05858037  0.12190776\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.07090168  0.29666963 -0.43370762 -0.21300772  0.06509455  0.13028355\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.07519837  0.2922218  -0.4363288  -0.19798651  0.07167981  0.13170514\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.07949515  0.28717455 -0.43632847 -0.2246568   0.07826505  0.13170476\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.0838953   0.28206146 -0.4463153  -0.2275904   0.08450264  0.12475183\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.08822574  0.27745995 -0.43995506 -0.20491603  0.09135152  0.13697742\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.0927515   0.2732621  -0.458857   -0.18696815  0.09758406  0.12465073\n",
      "  0.          0.        ] -> Action: 0\n",
      "Obs: [-0.09727736  0.26846474 -0.45885652 -0.21363807  0.10381658  0.12465038\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.10189982  0.26437834 -0.46850258 -0.18206562  0.11002375  0.12414348\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.10642929  0.25970057 -0.45683843 -0.20819326  0.1138868   0.07726104\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.1110487   0.25539258 -0.46575642 -0.19176061  0.11767711  0.07580616\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.11557398  0.25049424 -0.45395175 -0.2178146   0.11909259  0.02830966\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.12024517  0.24578112 -0.46816188 -0.20955482  0.12012609  0.02067008\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.12489414  0.24156633 -0.46645397 -0.18744993  0.12166792  0.03083684\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.12945089  0.23677856 -0.4548346  -0.21272275  0.12083601 -0.01663835\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.13407812  0.23232959 -0.4619144  -0.19766565  0.12002982 -0.01612381\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.13862352  0.22728439 -0.4516564  -0.22400115  0.11716923 -0.05721188\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.14316416  0.22237323 -0.4514815  -0.21807446  0.11459485 -0.05148749\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.14781094  0.21801224 -0.4619987  -0.19361787  0.1119398  -0.05310084\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.15238276  0.21306676 -0.4525968  -0.21946195  0.10737895 -0.09121721\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.15711728  0.20803429 -0.46827626 -0.22330187  0.10224018 -0.10277535\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.16196546  0.20327963 -0.47939056 -0.21095671  0.0968554  -0.10769562\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.1668621   0.19943412 -0.48445448 -0.17058508  0.09169897 -0.1031289\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.17167044  0.19501255 -0.4733692  -0.19607688  0.08429114 -0.14815646\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.17638913  0.18999726 -0.46210313 -0.22238186  0.07462369 -0.19334918\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.18123093  0.18500598 -0.4739759  -0.22135597  0.06453257 -0.20182224\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.18611518  0.1805297  -0.4782521  -0.19853544  0.05446749 -0.2013016\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.19091168  0.17545424 -0.4672342  -0.22517323  0.04219868 -0.24537627\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.19580288  0.17122622 -0.47645348 -0.18759947  0.02968131 -0.25034738\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.20060778  0.16639875 -0.46560764 -0.21432668  0.0149937  -0.2937524\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-2.0549646e-01  1.6253580e-01 -4.7367263e-01 -1.7160501e-01\n",
      " -8.1438538e-06 -3.0003679e-01  0.0000000e+00  0.0000000e+00] -> Action: 3\n",
      "Obs: [-0.21032152  0.15807067 -0.4656949  -0.19853576 -0.01660736 -0.33198446\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.21506552  0.15300079 -0.45552212 -0.22564074 -0.03524589 -0.37277076\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.21970001  0.14834605 -0.44498277 -0.20741105 -0.05348742 -0.36483085\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.22418757  0.14451708 -0.4307587  -0.17091669 -0.0712842  -0.35593557\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.22861376  0.14007977 -0.42308122 -0.19825344 -0.09063207 -0.3869576\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.23294416  0.135032   -0.4110977  -0.22581954 -0.11239993 -0.43535727\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.23702583  0.1309034  -0.38690525 -0.18523018 -0.13354875 -0.42297643\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.24102768  0.12617363 -0.37697768 -0.2124535  -0.15669475 -0.46292\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.24496102  0.12190118 -0.37002212 -0.19251002 -0.18003522 -0.46680927\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.24897332  0.11706863 -0.38009745 -0.21748282 -0.20127416 -0.4247786\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.25287303  0.1130122  -0.3685959  -0.18333669 -0.22284949 -0.4315068\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.25684828  0.10838493 -0.37821406 -0.20869133 -0.24243532 -0.3917166\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.26064116  0.10383948 -0.36037236 -0.20525746 -0.26170576 -0.38540882\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.26411074  0.09965301 -0.3288105  -0.18941103 -0.28029642 -0.37181336\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.26766014  0.09489928 -0.33893383 -0.21443501 -0.29676622 -0.3293962\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.2711709   0.09042822 -0.33456033 -0.20217617 -0.31386057 -0.34188697\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.27439705  0.08672427 -0.30613217 -0.16827996 -0.33100244 -0.34283784\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.2776987   0.08245111 -0.31570953 -0.19330324 -0.34612802 -0.30251125\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.28107196  0.07762743 -0.3249016  -0.21742795 -0.35919514 -0.26134276\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.2841776   0.07331815 -0.29815617 -0.19468671 -0.3723105  -0.2623068\n",
      "  0.          0.        ] -> Action: 1\n",
      "Obs: [-0.28735057  0.0684409  -0.3066706  -0.219573   -0.38359255 -0.22564141\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.29021162  0.06361443 -0.27607796 -0.2172469  -0.39428732 -0.21389572\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.2927199   0.05882336 -0.24154556 -0.21554418 -0.40422782 -0.19881022\n",
      "  0.          0.        ] -> Action: 2\n",
      "Obs: [-0.29485112  0.05411647 -0.20459919 -0.21165329 -0.4133944  -0.18333124\n",
      "  1.          0.        ] -> Action: 2\n",
      "Obs: [-0.29631108  0.05078774 -0.1547226  -0.14578646 -0.40321922  0.2047557\n",
      "  1.          0.        ] -> Action: 1\n",
      "Obs: [-0.29761273  0.04756573 -0.15867715 -0.13541098 -0.37275416  0.6093612\n",
      "  1.          0.        ] -> Action: 1\n",
      "Obs: [-0.2988143   0.04430474 -0.16442701 -0.13412079 -0.32612956  0.932598\n",
      "  1.          0.        ] -> Action: 1\n",
      "Obs: [-0.30001664  0.04077152 -0.174483   -0.14583376 -0.26996347  1.1234558\n",
      "  1.          0.        ] -> Action: 1\n",
      "Obs: [-0.30132818  0.03669213 -0.18699324 -0.17201999 -0.21314806  1.1363087\n",
      "  1.          0.        ] -> Action: 1\n",
      "Obs: [-0.30271047  0.03209354 -0.19682357 -0.19699937 -0.15428422  1.1772788\n",
      "  0.          1.        ] -> Action: 1\n",
      "Obs: [-0.30410567  0.02762762 -0.18698516 -0.19484481 -0.11103284  0.86008155\n",
      "  0.          0.        ] -> Action: 3\n",
      "Obs: [-0.3054456   0.02320219 -0.15615126 -0.19519041 -0.08907367  0.43915844\n",
      "  0.          1.        ] -> Action: 3\n",
      "Obs: [-0.30670843  0.01878778 -0.12775996 -0.19612174 -0.08782247  0.02475571\n",
      "  0.          1.        ] -> Action: 3\n",
      "Obs: [-0.3077088   0.0149211  -0.09939007 -0.17187528 -0.0884513  -0.01257207\n",
      "  1.          1.        ] -> Action: 3\n",
      "Obs: [-0.30844855  0.01156791 -0.07210861 -0.14913277 -0.09030114 -0.03699196\n",
      "  1.          1.        ] -> Action: 3\n",
      "Obs: [-0.30890942  0.0086874  -0.04224517 -0.12825383 -0.09410434 -0.0760607\n",
      "  1.          1.        ] -> Action: 3\n",
      "Obs: [-0.30909085  0.00626347 -0.01220986 -0.10811262 -0.09999041 -0.11771989\n",
      "  1.          1.        ] -> Action: 3\n",
      "Obs: [-0.30901274  0.00428883  0.01534108 -0.08828209 -0.10745956 -0.14938222\n",
      "  1.          1.        ] -> Action: 3\n",
      "Final Reward: -100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating the Agent\n",
    "\n",
    "Run the agent on 100 problems and report the average reward."
   ],
   "id": "c7868d743319aa46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:13:33.360307Z",
     "start_time": "2025-09-29T03:13:32.712302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 1: VY < -0.3\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "15ff801473b214f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, -100, 100, -100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Average reward: -82.0\n",
      "Success rate: 9/100\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:31:11.255498Z",
     "start_time": "2025-09-29T03:31:09.932820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 2: VY < -0.2\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "1b981f83d91d19e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, -100, -100, -100, -100, 100, -100, -100, np.float64(-0.0806241681046842), 100, 100, -100, 100, -100, -100, 100, -100, 100, -100, 100, 100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, 100, -100, 100, 100, -100, -100, -100, -100, 100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, 100, -100, -100, -100, -100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, -100, 100, -100, -100, 100, -100, 100, 100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, 100, np.float64(1.234205915060316), -100, -100, 100, 100]\n",
      "Average reward: -29.988464182530443\n",
      "Success rate: 34/100\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T03:31:05.650307Z",
     "start_time": "2025-09-29T03:31:04.438895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.15\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "4efc63ac214ad042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, 100, 100, -100, 100, 100, 100, -100, -100, -100, -100, -100, 100, -100, -100, -100, 100, -100, -100, 100, -100, -100, -100, -100, -100, np.float64(-0.22023092004587738), -100, 100, -100, np.float64(-0.021794024895214648), 100, 100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, 100, 100, -100, -100, 100, 100, -100, -100, 100, -100, -100, -100, -100, -100, 100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, 100]\n",
      "Average reward: -50.00242024944941\n",
      "Success rate: 24/100\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.2 | -0.1<X & X>0.1 left right\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "65e379da11c44cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tàu đã hạ cánh tốt hơn và ttimf ve dich tot hon. Toi nen them rule khi chua o X bang 0 thi ko tat dong co day.",
   "id": "1f81bf2c96250520"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: VY < -0.2 | -0.1<X & X>0.1 left right| abs(X) > 0.1 main\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "c76ffd933b0d4c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Khong dap dat duoc hahaha, gan dat thi no bat len",
   "id": "3ffdad89da4acff9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: đổi thứ tự sang đưa về pad 0 -> canh angle -> use main\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "357bf604b45f3769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tau rớt liên tù tì =))))",
   "id": "724b8a0c4ce7f437"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:16:34.582571Z",
     "start_time": "2025-09-30T09:16:33.125543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: đổi thứ tự sang đưa về -> use main -> pad 0 -> canh angle\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "5866a0465f9f6705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Average reward: -94.0\n",
      "Success rate: 3/100\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Như hạch",
   "id": "4cf5a10b51e70f41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:43:20.646105Z",
     "start_time": "2025-09-30T09:43:19.304749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST 3: tắt v angle (roi nhanh hon do tan suat bat main bi giam) ket qua cao nhat len toi 41/100 du tau mat can bang nhieu hơn\n",
    "#toi se thu dieu chinh angle ve 0.15 cho ra ket qua tot hon rat nhieu\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "3adcda7fa9a0fc8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, -100, -100, 100, 100, -100, 100, 100, 100, 100, 100, 100, 100, 100, -100, 100, 100, -100, -100, -100, 100, 100, 100, 100, 100, -100, 100, -100, 100, -100, 100, 100, 100, 100, -100, 100, -100, -100, 100, 100, -100, -100, 100, 100, -100, 100, -100, 100, 100, -100, 100, -100, -100, 100, 100, 100, 100, 100, -100, -100, 100, 100, 100, 100, -100, 100, 100, 100, -100, 100, 100, 100, -100, 100, -100, 100, 100, -100, -100, -100, -100, -100, -100, -100, -100, 100, -100, 100, 100, -100, 100, 100, 100, -100, 100, -100, -100, -100, 100, 100]\n",
      "Average reward: 20.0\n",
      "Success rate: 60/100\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST 3: tắt v angle (roi nhanh hon do tan suat bat main bi giam) ket qua cao nhat len toi 41/100 du tau mat can bang nhieu hơn\n",
    "#toi se thu dieu chinh angle ve 0.15 cho ra ket qua tot hon rat nhieu\n",
    "#toi them lai dieu kien v angle nhung de sau dieu kien angle\n",
    "#toi them dieu kien Y < -0.1 thi chay main ->> khong on lam\n",
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "368d8b57076e65ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:52:38.326879Z",
     "start_time": "2025-09-30T10:52:36.447659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "d42ba1d32ad3c174",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, -100, -100, -100, 100, -100, -100, -100, 100, -100, 100, 100, 100, 100, -100, -100, 100, -100, 100, -100, -100, 100, -100, 100, -100, 100, 100, -100, -100, 100, 100, 100, -100, 100, -100, 100, np.float64(0.21965917964532025), 100, -100, -100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, -100, -100, 100, -100, 100, 100, 100, -100, 100, -100, 100, 100, 100, 100, -100, 100, 100, 100, 100, -100, 100, -100, 100, -100, 100, -100, 100, 100, 100, np.float64(0.10289941503260039), 100, -100, 100, 100, 100, -100, 100, 100, 100, -100, 100, 100, 100, 100, -100, -100, 100]\n",
      "Average reward: 28.00322558594678\n",
      "Success rate: 63/100\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_episode_test(agent_function):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Reset the environment to generate the first observation\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    # run one episode (max. 1000 steps)\n",
    "    for _ in range(1000):\n",
    "        # call the agent to select an action\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step (transition) through the environment with the action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return reward\n",
    "\n",
    "def run_episodes(agent_function, n=100):\n",
    "    \"\"\"Run multiple episodes with the given agent and return the rewards for each episode.\"\"\"\n",
    "    return [run_episode_test(agent_function) for _ in range(n)]\n",
    "\n",
    "rewards = run_episodes(rocket_agent_function)\n",
    "print(rewards)\n",
    "\n",
    "print(f\"Average reward: {np.average(rewards)}\")\n",
    "print(f\"Success rate: {np.sum(np.array(rewards) == 100)}/{len(rewards)}\")"
   ],
   "id": "68e6b06319e52d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cách lưu code bằng video",
   "id": "ee1f32ba90aa8a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:31:20.230089Z",
     "start_time": "2025-10-05T10:31:18.754269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def run_episode(agent_function, max_steps=1000, video_folder=\"result\"):\n",
    "    \"\"\"Run one episode in the LunarLander-v3 environment using the provided agent.\"\"\"\n",
    "\n",
    "    # Initialize environment with rgb_array for video recording\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder,\n",
    "        episode_trigger=lambda ep: True,  # record every episode\n",
    "        name_prefix=\"lunarlander\"\n",
    "    )\n",
    "\n",
    "    # Reset environment\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        # chọn action theo agent\n",
    "        action = agent_function(observation)\n",
    "\n",
    "        # step: thực hiện hành động\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"Final Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "reward = run_episode(rocket_agent_function)\n",
    "print(\"Reward:\", reward)\n"
   ],
   "id": "c7c4a323a9f6dc0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reward: -14.39198234813665\n",
      "Reward: -14.39198234813665\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Chon lan dap cao diem nhat",
   "id": "518f054c48add1a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T10:35:34.809398Z",
     "start_time": "2025-10-05T10:34:55.207525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def run_episode(agent_function, max_steps=1000, video_folder=\"tmp_videos\"):\n",
    "    \"\"\"Run one episode and return total reward + path to video.\"\"\"\n",
    "\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder,\n",
    "        episode_trigger=lambda ep: True,\n",
    "        name_prefix=\"lunarlander\"\n",
    "    )\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action = agent_function(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "def run_best(agent_function, n_episodes=10):\n",
    "    best_reward = float(\"-inf\")\n",
    "    best_video = None\n",
    "\n",
    "    # clean up old folder\n",
    "    if os.path.exists(\"tmp_videos\"):\n",
    "        shutil.rmtree(\"tmp_videos\")\n",
    "\n",
    "    os.makedirs(\"tmp_videos\", exist_ok=True)\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        reward = run_episode(agent_function, video_folder=\"tmp_videos\")\n",
    "        print(f\"Episode {ep}: reward={reward}\")\n",
    "\n",
    "        # find latest video file created\n",
    "        video_files = [f for f in os.listdir(\"tmp_videos\") if f.endswith(\".mp4\")]\n",
    "        video_files.sort(key=lambda f: os.path.getmtime(os.path.join(\"tmp_videos\", f)))\n",
    "        latest_video = os.path.join(\"tmp_videos\", video_files[-1])\n",
    "\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_video = \"best_episode.mp4\"\n",
    "            shutil.copy(latest_video, best_video)\n",
    "            print(f\"🔥 New best! Saved {best_video}\")\n",
    "\n",
    "    print(f\"Best reward = {best_reward}, video saved at {best_video}\")\n",
    "    return best_reward, best_video\n",
    "\n",
    "best_reward, best_video = run_best(rocket_agent_function, n_episodes=20)\n"
   ],
   "id": "5e772143cad78c35",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khanh\\Documents\\GitHub\\TTNTNC_nhom21\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001B[33mWARN: Overwriting existing videos at C:\\Users\\Khanh\\Documents\\GitHub\\TTNTNC_nhom21\\ex02_search\\lab02_01_agents\\tmp_videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: reward=256.4432880229661\n",
      "🔥 New best! Saved best_episode.mp4\n",
      "Episode 1: reward=5.141186139868651\n",
      "Episode 2: reward=-27.01626478986438\n",
      "Episode 3: reward=-12.023107924915209\n",
      "Episode 4: reward=203.6237965495681\n",
      "Episode 5: reward=176.32706090857894\n",
      "Episode 6: reward=-74.81329816388359\n",
      "Episode 7: reward=184.38826445630522\n",
      "Episode 8: reward=15.407102413766083\n",
      "Episode 9: reward=159.06687583427413\n",
      "Episode 10: reward=-10.729076552091684\n",
      "Episode 11: reward=255.33831146298547\n",
      "Episode 12: reward=188.82489018593623\n",
      "Episode 13: reward=229.99244680978526\n",
      "Episode 14: reward=96.71992716426885\n",
      "Episode 15: reward=6.908580215106568\n",
      "Episode 16: reward=-23.464949567347404\n",
      "Episode 17: reward=-488.626030470921\n",
      "Episode 18: reward=-168.71692311824646\n",
      "Episode 19: reward=4.404359005830301\n",
      "Best reward = 256.4432880229661, video saved at best_episode.mp4\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lay 10 episode diem cao va 10 episode diem thap, ghep thanh 1 video cho moi truong hop",
   "id": "9ba6c99c5acde497"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T11:02:51.781048Z",
     "start_time": "2025-10-05T11:02:04.205288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def run_episode(agent_function, env, max_steps=1000):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action = agent_function(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def record_multiple(agent_function, n_total=100, max_steps=1000):\n",
    "    results = []\n",
    "    for i in range(n_total):\n",
    "        video_dir = f\"videos/episode_{i}\"\n",
    "        env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        env = gym.wrappers.RecordVideo(env, video_dir, episode_trigger=lambda x: True)\n",
    "\n",
    "        reward = run_episode(agent_function, env, max_steps)\n",
    "        env.close()\n",
    "        results.append((reward, video_dir))\n",
    "        print(f\"Episode {i} finished with reward {reward}\")\n",
    "    return results\n",
    "\n",
    "def split_best_worst(results, top_k=10):\n",
    "    sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "    worst = sorted_results[:top_k]\n",
    "    best = sorted_results[-top_k:]\n",
    "\n",
    "    # Tạo folder lưu riêng\n",
    "    os.makedirs(\"videos_best\", exist_ok=True)\n",
    "    os.makedirs(\"videos_worst\", exist_ok=True)\n",
    "\n",
    "    for reward, folder in best:\n",
    "        for f in os.listdir(folder):\n",
    "            if f.endswith(\".mp4\"):\n",
    "                shutil.copy(os.path.join(folder, f), f\"videos_best/best_{reward:.2f}.mp4\")\n",
    "\n",
    "    for reward, folder in worst:\n",
    "        for f in os.listdir(folder):\n",
    "            if f.endswith(\".mp4\"):\n",
    "                shutil.copy(os.path.join(folder, f), f\"videos_worst/worst_{reward:.2f}.mp4\")\n",
    "\n",
    "    print(\"Saved top 10 best episodes in videos_best/, worst 10 in videos_worst/\")\n",
    "\n",
    "results = record_multiple(random_agent_function, n_total=100)  # chạy 100 tập\n",
    "split_best_worst(results, top_k=10)\n"
   ],
   "id": "b2ae99acbe7e438",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished with reward -201.8955687383574\n",
      "Episode 1 finished with reward -380.5658767263654\n",
      "Episode 2 finished with reward -318.09908811841126\n",
      "Episode 3 finished with reward -130.78199365014902\n",
      "Episode 4 finished with reward -240.1892674541233\n",
      "Episode 5 finished with reward -139.3406049690797\n",
      "Episode 6 finished with reward -79.69014950795855\n",
      "Episode 7 finished with reward -114.35685263095512\n",
      "Episode 8 finished with reward -367.50153686176634\n",
      "Episode 9 finished with reward -76.79052059388039\n",
      "Episode 10 finished with reward -143.075744181998\n",
      "Episode 11 finished with reward -134.70659441460032\n",
      "Episode 12 finished with reward -161.81856541199943\n",
      "Episode 13 finished with reward -84.30771795632651\n",
      "Episode 14 finished with reward -273.3719064499886\n",
      "Episode 15 finished with reward -452.658347368328\n",
      "Episode 16 finished with reward -122.8493251738608\n",
      "Episode 17 finished with reward -103.10257401718539\n",
      "Episode 18 finished with reward -301.33504099324176\n",
      "Episode 19 finished with reward -222.09909256899488\n",
      "Episode 20 finished with reward -123.37605931936574\n",
      "Episode 21 finished with reward -124.23487202665069\n",
      "Episode 22 finished with reward -144.80195358842482\n",
      "Episode 23 finished with reward -82.20854640522512\n",
      "Episode 24 finished with reward -300.93540270043627\n",
      "Episode 25 finished with reward -116.78127820245152\n",
      "Episode 26 finished with reward -300.31242874232134\n",
      "Episode 27 finished with reward -97.23722361591263\n",
      "Episode 28 finished with reward -24.338206559060282\n",
      "Episode 29 finished with reward -147.07847652300205\n",
      "Episode 30 finished with reward -320.12103908974973\n",
      "Episode 31 finished with reward -122.76573911784062\n",
      "Episode 32 finished with reward -233.07155670276245\n",
      "Episode 33 finished with reward -103.27052768100982\n",
      "Episode 34 finished with reward -93.36339629645278\n",
      "Episode 35 finished with reward -94.87530607334581\n",
      "Episode 36 finished with reward -147.82595520232044\n",
      "Episode 37 finished with reward -315.99172395406345\n",
      "Episode 38 finished with reward -290.2438742469214\n",
      "Episode 39 finished with reward -273.90688056833864\n",
      "Episode 40 finished with reward -124.22594138935247\n",
      "Episode 41 finished with reward -78.89282737983874\n",
      "Episode 42 finished with reward -177.71805594465866\n",
      "Episode 43 finished with reward -185.05463611789202\n",
      "Episode 44 finished with reward -362.08370711323226\n",
      "Episode 45 finished with reward -207.88564188408682\n",
      "Episode 46 finished with reward -25.355111833051538\n",
      "Episode 47 finished with reward -230.92878860264727\n",
      "Episode 48 finished with reward -81.11989108784738\n",
      "Episode 49 finished with reward -129.39926431683148\n",
      "Episode 50 finished with reward -368.93583964956923\n",
      "Episode 51 finished with reward -131.7293882860902\n",
      "Episode 52 finished with reward -110.7323717208639\n",
      "Episode 53 finished with reward -133.62330609371446\n",
      "Episode 54 finished with reward -79.14437665984818\n",
      "Episode 55 finished with reward -110.25542970272272\n",
      "Episode 56 finished with reward -286.66689398420095\n",
      "Episode 57 finished with reward -186.39097774939137\n",
      "Episode 58 finished with reward -75.97053892366833\n",
      "Episode 59 finished with reward -379.56670222703394\n",
      "Episode 60 finished with reward -377.65375621017455\n",
      "Episode 61 finished with reward -487.3172315249556\n",
      "Episode 62 finished with reward -103.65110197439972\n",
      "Episode 63 finished with reward -154.63486621237632\n",
      "Episode 64 finished with reward -136.79790471318063\n",
      "Episode 65 finished with reward -270.3663156046068\n",
      "Episode 66 finished with reward -164.39225495539412\n",
      "Episode 67 finished with reward -67.05075208119992\n",
      "Episode 68 finished with reward -342.27455626017303\n",
      "Episode 69 finished with reward -135.75025823284057\n",
      "Episode 70 finished with reward -196.81177767923737\n",
      "Episode 71 finished with reward -61.42548590682786\n",
      "Episode 72 finished with reward -401.98091785616384\n",
      "Episode 73 finished with reward -256.8105494891619\n",
      "Episode 74 finished with reward -378.74707722503234\n",
      "Episode 75 finished with reward -77.52560104541156\n",
      "Episode 76 finished with reward -137.61566098747255\n",
      "Episode 77 finished with reward -344.2444922502535\n",
      "Episode 78 finished with reward -108.68202083004525\n",
      "Episode 79 finished with reward -179.66311206012114\n",
      "Episode 80 finished with reward -105.97843317083554\n",
      "Episode 81 finished with reward -73.38519616762783\n",
      "Episode 82 finished with reward -121.16753367219387\n",
      "Episode 83 finished with reward -113.60411439456493\n",
      "Episode 84 finished with reward -383.55299518950517\n",
      "Episode 85 finished with reward -325.37897719587465\n",
      "Episode 86 finished with reward -437.5743622548857\n",
      "Episode 87 finished with reward -183.56609170162216\n",
      "Episode 88 finished with reward -120.39281732644451\n",
      "Episode 89 finished with reward -139.94271142061424\n",
      "Episode 90 finished with reward -118.50143015361539\n",
      "Episode 91 finished with reward -136.7394105603052\n",
      "Episode 92 finished with reward -104.28810172898923\n",
      "Episode 93 finished with reward -87.01851312143893\n",
      "Episode 94 finished with reward -116.93657641098514\n",
      "Episode 95 finished with reward -86.52635481389873\n",
      "Episode 96 finished with reward -117.11691936489662\n",
      "Episode 97 finished with reward -342.23070638154195\n",
      "Episode 98 finished with reward -302.5878274581549\n",
      "Episode 99 finished with reward -83.4372039073086\n",
      "Saved top 10 best episodes in videos_best/, worst 10 in videos_worst/\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
